{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fmodern\fcharset0 CourierNewPS-ItalicMT;\f1\fnil\fcharset0 Menlo-Regular;\f2\fswiss\fcharset0 Helvetica-Oblique;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red217\green222\blue223;\red157\green0\blue210;
\red255\green255\blue254;\red15\green112\blue1;\red0\green0\blue255;\red32\green108\blue135;\red101\green76\blue29;
\red0\green0\blue109;\red144\green1\blue18;\red19\green120\blue72;\red108\green121\blue122;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c87843\c89412\c89804;\cssrgb\c68627\c0\c85882;
\cssrgb\c100000\c100000\c99608;\cssrgb\c0\c50196\c0;\cssrgb\c0\c0\c100000;\cssrgb\c14902\c49804\c60000;\cssrgb\c47451\c36863\c14902;
\cssrgb\c0\c6275\c50196;\cssrgb\c63922\c8235\c8235;\cssrgb\c3529\c53333\c35294;\cssrgb\c49804\c54902\c55294;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl360\partightenfactor0

\f0\i\fs32 \cf2 \cb3 \expnd0\expndtw0\kerning0
pip freeze | xargs pip uninstall -y\
\pard\pardeftab720\sl380\partightenfactor0

\f1\i0\fs28 \cf4 \cb5 \outl0\strokewidth0 \strokec4 import\cf2 \strokec2  tensorflow \cf4 \strokec4 as\cf2 \strokec2  tf\cb1 \
\cf4 \cb5 \strokec4 import\cf2 \strokec2  tensorflow_hub \cf4 \strokec4 as\cf2 \strokec2  hub\cb1 \
\cf4 \cb5 \strokec4 import\cf2 \strokec2  bert\cb1 \
\cf4 \cb5 \strokec4 from\cf2 \strokec2  bert \cf4 \strokec4 import\cf2 \strokec2  tokenization\cb1 \
\cf4 \cb5 \strokec4 from\cf2 \strokec2  bert \cf4 \strokec4 import\cf2 \strokec2  run_classifier\cb1 \
\cf4 \cb5 \strokec4 from\cf2 \strokec2  bert \cf4 \strokec4 import\cf2 \strokec2  optimization\cb1 \
\cf6 \cb5 \strokec6 # from kneed import KneeLocator\cf2 \cb1 \strokec2 \
\cf6 \cb5 \strokec6 # from sklearn.metrics import pairwise_distances_argmin_min\cf2 \cb1 \strokec2 \
\cf4 \cb5 \strokec4 import\cf2 \strokec2  pandas \cf4 \strokec4 as\cf2 \strokec2  pd\cb1 \
\cf4 \cb5 \strokec4 import\cf2 \strokec2  numpy \cf4 \strokec4 as\cf2 \strokec2  np\cb1 \
\cf4 \cb5 \strokec4 import\cf2 \strokec2  math\cb1 \
\cf4 \cb5 \strokec4 import\cf2 \strokec2  itertools\cb1 \
\cf4 \cb5 \strokec4 from\cf2 \strokec2  sklearn.metrics.pairwise \cf4 \strokec4 import\cf2 \strokec2  cosine_similarity\cb1 \
\cf6 \cb5 \strokec6 # from utils.stats_utils import *\cf2 \cb1 \strokec2 \
\cf4 \cb5 \strokec4 from\cf2 \strokec2  nlp_utils \cf4 \strokec4 import\cf2 \strokec2  *\cb1 \
\
\cf7 \cb5 \strokec7 class\cf2 \strokec2  \cf8 \strokec8 BERT\cf2 \strokec2 (\cf8 \strokec8 object\cf2 \strokec2 ):\cb1 \
\cb5     \cf7 \strokec7 def\cf2 \strokec2  \cf9 \strokec9 __init__\cf2 \strokec2 (\cf10 \strokec10 self\cf2 \strokec2 ):\cb1 \
\cb5         \cb1 \
\cb5         \cf10 \strokec10 self\cf2 \strokec2 .ABBREVIATION = [\cf11 \strokec11 "Sr."\cf2 \strokec2 , \cf11 \strokec11 "i.e."\cf2 \strokec2 ]\cb1 \
\cb5         \cf10 \strokec10 self\cf2 \strokec2 .SKIP = []\cb1 \
\cb5         \cf10 \strokec10 self\cf2 \strokec2 .BERT_URL = \cf11 \strokec11 'https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1'\cf2 \cb1 \strokec2 \
\cb5         \cf10 \strokec10 self\cf2 \strokec2 .bert_module = hub.Module(\cf10 \strokec10 self\cf2 \strokec2 .BERT_URL)   \cb1 \
\cb5     \cb1 \
\cb5     \cf6 \strokec6 # return a tuple (text, indicator, gap_score). \cf2 \cb1 \strokec2 \
\cb5     \cf6 \strokec6 # If indicator is false, no best k was found. \cf2 \cb1 \strokec2 \
\cb5     \cf6 \strokec6 # If gap_score was not positive, summarization is not valid. \cf2 \cb1 \strokec2 \
\cb5         \cb1 \
\cb5          \cb1 \
\cb5     \cf7 \strokec7 def\cf2 \strokec2  \cf9 \strokec9 bert_embeddings\cf2 \strokec2 (\cf10 \strokec10 self\cf2 \strokec2 , \cf10 \strokec10 sentences\cf2 \strokec2 ):\cb1 \
\cb5         \cf6 \strokec6 # input: a list of string\cf2 \cb1 \strokec2 \
\cb5         module = \cf10 \strokec10 self\cf2 \strokec2 .bert_module\cb1 \
\cb5         tokenizer = \cf10 \strokec10 self\cf2 \strokec2 .create_tokenizer_from_hub_module()\cb1 \
\
\cb5         input_ids_vals, input_mask_vals, segment_ids_vals = convert_sentences_to_features(sentences = sentences, \\\cb1 \
\cb5                                                             max_seq_len = \cf12 \strokec12 128\cf2 \strokec2 , \\\cb1 \
\cb5                                                             tokenizer = tokenizer)\cb1 \
\cb5         input_ids = tf.placeholder(dtype=tf.int32, shape=[\cf7 \strokec7 None\cf2 \strokec2 , \cf7 \strokec7 None\cf2 \strokec2 ])\cb1 \
\cb5         input_mask = tf.placeholder(dtype=tf.int32, shape=[\cf7 \strokec7 None\cf2 \strokec2 , \cf7 \strokec7 None\cf2 \strokec2 ])\cb1 \
\cb5         segment_ids = tf.placeholder(dtype=tf.int32, shape=[\cf7 \strokec7 None\cf2 \strokec2 , \cf7 \strokec7 None\cf2 \strokec2 ])\cb1 \
\
\cb5         bert_inputs = dict(\cb1 \
\cb5             input_ids=input_ids,\cb1 \
\cb5             input_mask=input_mask,\cb1 \
\cb5             segment_ids=segment_ids)\cb1 \
\
\cb5         bert_outputs = \cf10 \strokec10 self\cf2 \strokec2 .bert_module(bert_inputs, signature=\cf11 \strokec11 "tokens"\cf2 \strokec2 , as_dict=\cf7 \strokec7 True\cf2 \strokec2 )\cb1 \
\
\cb5         sess = tf.Session()\cb1 \
\cb5         sess.run(tf.global_variables_initializer())\cb1 \
\cb5         out = sess.run(bert_outputs, feed_dict=\{input_ids: input_ids_vals, \\\cb1 \
\cb5                                                 input_mask: input_mask_vals, segment_ids: segment_ids_vals\})\cb1 \
\
\cb5         \cf4 \strokec4 return\cf2 \strokec2  out[\cf11 \strokec11 'pooled_output'\cf2 \strokec2 ]\cb1 \
\
\
\cb5     \cf7 \strokec7 def\cf2 \strokec2  \cf9 \strokec9 create_tokenizer_from_hub_module\cf2 \strokec2 (\cf10 \strokec10 self\cf2 \strokec2 ):\cb1 \
\cb5         tokenization_info = \cf10 \strokec10 self\cf2 \strokec2 .bert_module(signature=\cf11 \strokec11 "tokenization_info"\cf2 \strokec2 , as_dict=\cf7 \strokec7 True\cf2 \strokec2 )\cb1 \
\cb5         \cf4 \strokec4 with\cf2 \strokec2  tf.Session() \cf4 \strokec4 as\cf2 \strokec2  sess:\cb1 \
\cb5             vocab_file, do_lower_case = sess.run([tokenization_info[\cf11 \strokec11 "vocab_file"\cf2 \strokec2 ], tokenization_info[\cf11 \strokec11 "do_lower_case"\cf2 \strokec2 ]])\cb1 \
\cb5         \cf4 \strokec4 return\cf2 \strokec2  tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\cb1 \
\cb5     \cb1 \
\cb5     \cf7 \strokec7 def\cf2 \strokec2  \cf9 \strokec9 batch\cf2 \strokec2 (\cf10 \strokec10 self\cf2 \strokec2 ,\cf10 \strokec10 iterable\cf2 \strokec2 , \cf10 \strokec10 n\cf2 \strokec2 =\cf12 \strokec12 1\cf2 \strokec2 ):\cb1 \
\cb5         l = \cf9 \strokec9 len\cf2 \strokec2 (iterable)\cb1 \
\cb5         \cf4 \strokec4 for\cf2 \strokec2  ndx \cf7 \strokec7 in\cf2 \strokec2  \cf9 \strokec9 range\cf2 \strokec2 (\cf12 \strokec12 0\cf2 \strokec2 , l, n):\cb1 \
\cb5             \cf4 \strokec4 yield\cf2 \strokec2  iterable[ndx:\cf9 \strokec9 min\cf2 \strokec2 (ndx + n, l)]\cb1 \
\cb5     \cb1 \
\
\cb5     \cb1 \
\pard\pardeftab720\sl380\partightenfactor0
\cf2 \outl0\strokewidth0 \
\pard\pardeftab720\sl360\partightenfactor0

\f2\i\fs32 \cf13 \
}